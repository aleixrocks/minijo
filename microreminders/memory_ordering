DEFINITIONS
 - Best documentation ever for beginners:
    - LKMM: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/tools/memory-model/Documentation/explanation.txt
    - memory barriers: https://www.kernel.org/doc/Documentation/memory-barriers.txt
    - atomics and specially atomic_mb__{after,before}: https://www.kernel.org/doc/Documentation/atomic_t.txt
 - LKMM: Linux Kernel Memory Model. Explains what each kernel API related to
   ordering does and defines a model in code to run simulations.
 - A memory model for an ISA is the specification of all legal multithreaded
   program behaviors. 
     - Sequencial Consistency
     - Total Store Order: MCA and rMCA
     - RVWMO: is a type of rMCA
     - Relaxed memory models: non-MCA
 - litmus test: code that examplifies a particular tricky case where memory
   ordering matters. Typical codes are MP (message passing) SB (store buffer)
   IRIW (independent reads of independent writes)
 - plain access vs marked access: a plain access is "p = a", while a marked
   access uses a function like "p = READ_ONCE(a)"
 - Program order is the order in which memory access instructions appear in each thread.
 - Global Memory order is the order in which accesses from different cores read and write memory
 - Preserved program order (ppo) represents the subset of program order that
   must be respected within the global memory order. Conceptually, events from
   the same hart that are ordered by preserved program order must appear in
   that order from the perspective of other harts and/or observers. Events from
   the same hart that are not ordered by preserved program order, on the other
   hand, may appear reordered from the perspective of other harts and/or
   observers.
 - Store atomicity disallows a core to see its own stores before they are
   written to memory, trading off performance for a more intuitive memory mode
 - A memory operation "performs" (enters the global memory order) when:
    - a load determines its return value
    - a store becomes globally visible
 - If we have X; Y; then, Y is the younger instruction and X is the older.
 - coherence order (co): Unique order of writes that goes from first to last
   for each memory location.
 - internal coherence order (coi): co link within the same cpu
 - external coherence order (coe): co link between cpus
 - relations between write and read 
   - read-from (rf): a load reads a value from a previous write
   - internal read-from (rfi): rf link on the same cpu
   - external read-from (rfe): rf link between cpus
   - from-read (fr): a load reaturns the intial value despite that memory
     location was written by a previous write.
   - from-read (fr): R fr-> W if the value that R read time ago, is now
     overwritten by W for another value. If there are multiple W on that
     variable after the R, there is a "fr->" for each of the writes (until
     another load aprears I think). Sometimes is depicted in the diagrams as
     the "this is what should have happened but instead we got something
     totally differnet!"
   - internal from-read (fri): fr where both w and r are in the same cpu
   - external from-read (fre): fr where w and r are in different cpus
 - Release consistency:
    - RCsc: Release Consistency of sequential consistency: a release operation
      orders all memory operations before it.
    - RCpc: Release Consistency of processor consistency: same as RCsc but
      excludes ordering the write->read memory operations.
    - RCtso: Release Consistency of total store order: same as RCpc but
      enforces MCA
 - Syntatic dependecy vs Semantinc dependecy. Syntactically, the following code
   has a data dependecy. However, semantically it does not, because r * 0 can
   be optimized away! Normally this is something that the comiler can do, but
   not the processor (RISC-V surely doesn't do it)
     r = READ_ONCE(y)
     WRITE_ONCE(x, r * 0) // the compiler might change r * 0 for 0, removing the dependency! 
 - A-cumulative: This is a property that some fences have (such as smp_mb() or
   release operation in the linux kernel). Say cpu C0 generates a store Wc0,
   and the store is seen by cpu C1, and cpu C1 generates the store C1s. Then,
   if cpu C1 uses a fence with the A-cumulative property, any other cpu CX will
   see first the Wc0 store and then the Wc1 store (IOW, in order). If not using
   a A-cumlative fence, processors using a weak memory model (such as Power)
   might see Wc1 before Wc0.
 - cumul-fence: This is a property that (I think) all fences have. If there is
   a fence with  the cumul-fence property between store W and W'. Then W and W'
   must be seen in order in all cores. However, each core can see W and W' with
   some delay.

Rationale
 - why we need barriers in both a read and write side of a synchronization?
    - A barrier on the write side enforces that all previous operations have
      been flushed from the store buffer to the cache coherency system.
    - On the read side to prevent processor speculation. The processor might
      perform reads speculation in the sense that it reads a po-later variable
      before a po-earlier variable because it already has the cache line for
      the po-later variable, for example. Also, Some processors also have
      invalidation queues which contains requests to invalidate cache lines
      that have not yet been applied. A read barrier ensures that all
      invalidations have been processed before continuing.

Dependencies enforcing ordering
 - Three type of dependencies between instructions: control (if), data and address
 - For one of these dependencies to exist, the first instruction must be a
   load, the second can be either a load or a store. Only stores generate data,
   so an instruction can only depend on a load!
 - Given memory instructions X followed by Y (first X, then Y), ppo is followed if:
    - Y is a store:
       - X and Y share a data, address or control dependency. Think that once a
         store is done, we can't go back!  We can't speculate neither the
         value, addres or whether the store is done or not!
    - Y is a load
       - There is an address dependency with X (any cpu can load something
         without knowing the address). But beaware of alpha, they have a
         split-cache design that makes loads look like they are out-of-order
         even if there is an address dependency.
       - Also note:
          - If Y is a load, there can't be a data dependency!
          - If Y is a load and there is a control dependency, nothing prevents
            the cpu from running it speculatively and the discarding the result
            if the "if branch" is not taken. Thus this case does not guarantee
            ppo.
 - If we have the case R ->dep W ->rfi R'
    - if dep is a value depdency, R' indirectly depends on R because W can't
      forward to R it's value if it doesn't know it!
    - if dep is an addres depdency, R' indirectly depends on R because W can't
      forward to R if we don't know if W and R' share the same address. Note
      that if W and R' use the same register, a processor could know that the
      address is the same even if it doesn't know the address itself. However,
      no processor does forwarding in these cases. That's possibly because of
      illegal accesses interruptions.
    - if dep is a control dependency, R' could run before R and W
      speculatively. If the branch is not take, it can be discarded.
 - If X and Y share the same memory location:
    - If Y is a store, ppo is always followed. Otherwise the store could
      overwrite the memory contens before a load could read it or a older store
      could overwrite a younger one!
 - Control dependencies pair normally with other barriers. Note that a real control dependency must be of the form:
    r1 = READ_ONCE(a);
    if (r1)
      WRITE_ONCE(b,1);
   In essence, a read, followed by a write. Nothing else works. This code
   ensures that the read happens before the write. There is no need to flush
   the store-buffer to the cache between the load and the store, that's why it
   works. But note that the only effect is the read and write to be ordered!
   (you still need a strong barrier after the write if you would like the write
   to be visible to ALL cores).
 - Data dependencies between loads are not data barriers.
    r1 = load A
    r2 = load r1 /*pointer dereference*/
   This might look out of order, even if there is a write barrier on the write
   side. I think this only happens in alpha though.



Defining a memory model
 - When defining a memory model, it can be done using the "axiomatic" or the "operational" approaches:
    - axiomatic: defines a collection of constraints on legal program behaviors. 
    - operational: defines an abstract machine that can run a program and directly produce its legal behaviors
 - In both cases we need to know:
    - Global memory order: when is respected?
    - Preserved Program order: when is respected?
    - Load Value Axiom: which value a load instruction will return?
 - Simple definitions source: RISC-V Memory Consistency Model Tutorial by Dan Lusting 2017
 - Sequential consistency:
    - axiomatic:
      - Global memory order: There is a total order on all memory operations. The
        order is non-deterministic.
      - Preserved program Order: That total order respects program order
      - Load Value Axiom: Loads return the value written by the latest store to
        the same address in the total order
    - operational:
      - Harts take turn executing instructions. The order is nondeterministic.
      - Each hart executes its own instructions in order
      - Loads return the value written by the most recent preceding store to the
        same address
 - Total Store Order (SPARC, x86, RVTSO)
    - Axiomatic
       - There is a total order on all memory operations. The order is
         non-deterministic.
       - That total order respects program order, except Store->Load ordering
       - Loads return the value written by the latest store to the same address in
         program or memory order (whichever is later)
    - Operational
       - Harts take turn executing steps. The order is non-deterministic.
       - Each hart executes its own instructions in order
       - Stores execute in two steps: 1) enter store buffer, 2) drain to memory
       - Loads first try to forward from the store buffer. If that fails, they return
         the value written by the most recent preceding store to the same address
  - RISC-V WEAK MEMORY ORDERING (RVWMO)
     - Axiomatic
       - There is a total order on all memory operations. The order is
         non-deterministic.
       - That total order respects thirteen specific patterns (next slide)
       - Loads return the value written by the latest store to the same address in
         program or memory order (whichever is later)
     - Operational
       - Harts take turn executing steps. The order is non-deterministic.
       - Each hart executes its own instructions in order
       - Multiple steps for each instruction (see spec Appendix B in RISCV spec)
       - Loads first try to forward from the store buffer. If that fails, they return
         the value written by the most recent preceding store to the same address

Memory Models
 - Total Store Order (TSO) relaxes the store→load order with the express
   purpose of accommodating a store buffer. The store buffer is a critical
   component for performance. It allows a core to retire its store instructions
   and continue executing without having to wait for the stores to write
   memory. In TSO, a younger load bypasses older unperformed stores (on
   different addresses) in the store buffer, hence the store→load order is
   relaxed. 
 - TSO implementations can come in different flavors. These flavors
   differentiate in store atomicity guarantees. We say that store atomicity is
   guaranteed if all cores agree in the memory order of stores.  For example,
   IBM 370 systems and their presentday descendants, the z/Architecture series
   [23] opt for respecting store atomicity. We refer to this store-atomic TSO
   memory model as the 370 model. In the IBM 370, store atomicity is achieved
   by requiring that a store in limbo, in the store buffer, must first be
   inserted in memory order before it can be forwarded to any local load. This
   means that whenever a load matches a store in the store buffer, the load is
   not performed until the store buffer is drained in the memory system (at
   least up to the matched store).
 - On the other hand, x86 and SPARC go a step further. They relax store
   atomicity by allowing a core to see its own stores while they are in limbo,
   i.e., executed (and perhaps retired) but not yet inserted in memory order.
   This is known as store-to-load forwarding and complicates the memory model.
   In this case, it is the task of the software to guarantee memory ordering
   when needed.
   - Types of store atomicity:
    - Multi-copy atomic (MCA) or store-atomicity: All cores see the new value of a store at the same time.
    - Other multy-copy atomic or read-own-write-early Multi-copy-atomic (rMCA):
      A store becomes visible to the issuing processor before it is advertised
      simultaneously to all other processors, e.g., in TSO and Alpha.
    - Non-atomic (or non-multi-copy-atomic): a store becomes visible to
      different processors at different times, e.g., in POWER and ARM.
    - NOTE: the definition of multi-copy atomiciy is ambiguous. There's
      literaturte that refer to the following (RISC-V uses this T_T):
       - Single-copy atomic: a store becomes visible to all processors at the
         same time, e.g., in SC.
       - Multi-copy atomic: a store becomes visible to the issuing
         processor before it is advertised simultaneously to all other
         processors, e.g., in TSO and Alpha [4].
       - Non-atomic (or non-multi-copy-atomic): a store becomes visible to
         different processors at different times, e.g., in POWER and ARM.
      See "Weak Memory Models: Balancing Definitional Simplicity and
      Implementation Flexibility" and Speculative Enforcement of Store
      Atomicity" and "perfbook" for info on the definitions.
   - NOTE2: Sometimes, the term TSO alone is used to denote TSO+rMCA


Arquitectures
 - RVWMO: is a type of rMCA. Still, each core might reorder loads and stores with more flexibility than TSO allows.
 - x86, x86_64 (or amd64) and SPARC are TSO
 - POWER and ARM are weakly ordered but they have more coherent options


C11 Memory model
 - main doc:
    - https://en.cppreference.com/w/cpp/atomic/memory_order
    - https://en.cppreference.com/w/c/atomic/memory_order
    - the official c spec is unreadable c'mon!
 - acquire-release pairs synchronize variables seen by the involves cpus, but
   not any other cpu! Hence, C11 model does not enforce other-MCA (stores can be
   seen in diferent orders in diferent cpus)
 - memory_order_seq_cst acts as a full barrier (a store acts as a release and a
   load as an acqurie) but also enforces total store ordering among all
   operations tagged seq_cst (all cpus see these values in the same order).
   Essentially, this means that a seq_cst store is propagated to all cpus
   before continuing, and two seq_cst stores will be seen in all cpus in the
   same order. This is what the sync (power) and dmb (arm) instructions do. See the final example under
   https://en.cppreference.com/w/cpp/atomic/memory_order#Sequentially-consistent_ordering
 - An atomic RMW operation tagged with memory_order_acq_release or memory_order_seq_cst won't allow:
    - subsequent operations to be seen before the load (R) part
    - previous operations to be seen after the store (W) part
   However, previous and subsequent operations might get mixed in between the R
   and W part! And between them, you might end up performing a subsequent
   operation before a previuos operation! See cppmem example below. Note that
   two seq_cst operations won't get interwined.
 - cppmem tool: memory model simulator: http://svr-pes20-cppmem.cl.cam.ac.uk/cppmem/
    - x.load(memory_order_relaxed).readsvalue(0): Only considers executions
      where this loads of x ends up being 0.
    - r1 = x.load(memory_order_relaxed):  stores read value in register r1, shown in final graph
    - {{{ { x = 1; r1 = y; } ||| { y = 1; r2 = x; } }}} defines two threads for simplicity
    - cas_strong(), and cas_strong_explicit are aliases for atomic_compare_exchange_strong
    - more accepted functions: https://github.com/goodell/cppmem/blob/master/cil-parser/clexer.mll#L169
    - example code:
       int main() {
         atomic_int x=0; atomic_int y=0;
         {{{ { 
               cas_strong_explicit(x,0,1,memory_order_acq_rel,memory_order_relaxed);
               y.store(1,memory_order_relaxed); 
         } ||| { 
               r1=y.load(memory_order_acquire).readsvalue(1);
               r2=x.load(memory_order_acquire);
         } }}}
         return 0;
       }
      The outcome r1=1 and r2=0 is allowed, because with acq_rel or seq_cst in
      cas_strong, the y store can be mixed between the R and W part of the CAS.
      Changing the y.store to memory_order_release will prevent this outcome.

Kernel functions
 - READ_ONCE and WRITE_ONCE prevent the compiler from removing the associated
   load and store instructions and to reorder them (at compiler level!!!). This
   means that the compiler won't reorder READ_ONCE or WRITE_ONCE calls with
   other READ_ONCE, WRITE_ONCE, barrier or similar calls, but can reorder them
   with other plain memory accesses. READ_ONCE() and WRITE_ONCE() are
   implemented as volatile casts, which has no effect when its argument is
   already marked volatile. Hence, do not use *_ONCE when the variable is
   volatile. Next follows some exeptions when the compiler might still reorder
   *_ONCE accesses:
     - the following code
         r = READ_ONCE(y)
         if (r) {
           WRITE_ONCE(x, 2)
         } else {
           WRITE_ONCE(x, 2)
         }
       can be changed by 
         r = READ_ONCE(y)
         WRITE_ONCE(x, 2) // the processor can now reorder them as there is no control dependency!
         if (r) {
            ...
         } else {
            ...
         }
     - it can also optimize away the values of such instructions removing dependencies
         r = READ_ONCE(y)
         WRITE_ONCE(x, r * 0) // the compiler might change r * 0 for 0, removing the dependency! 
     - the compiler assumes that r1 is undefined because i is not initialized,
       so it can assume that r1 is always 0 and break the dependecy! The LKMM is unaware of this!
        int a[1];
        int i;
        r1 = READ_ONCE(i);
        r2 = READ_ONCE(a[r1]);
 - spin_lock and spin_unlock vs load_acquire and store_release (see LKMM for details)
    - the spin_lock uses an rmw whose load part has acquire semantincs similar
      to load_acquire. spin_unlock uses store_relase. But they are not exaclty
      the same for two reasons. The load acquire part in a spin_lock is named
      lock_acquire and the release part in spin_unlock is named lock_release.
       - critical sections protected by different locks cannot be reordered.
         For instance:
           write X; spin_lock(s); spin_unlock(t); write Y;
         But the same code with acquire and release can lead to x and y
         reordered!! I think that if the spinlock variable is the same, it can
         be reordered.
       - a spin_release paired with a spin_lock with the same variable on
         different cpus guarantees that writes before the spin_release are seen
         before writes after the spin_acquire on __ALL__ cpus. Instead, when using
         load_acquire and store_release, only the cpus involved get the
         variables in order, but others might not!
 - barrier(): this is just a compiler barrier, not a processor one! This calls
   ensures ordering of all previous memory operations with the following memory
   operations. As a consequence, the compiler must discard the value of all
   memory locations that it has currently cached in any machine registers,
   which means it is somewhat expensive.
 - atomic functions
    - Be aware of atomics that do not return value, such as atomic_inc. In the
      following code, it is possible for X and Y to be reordered. That's because
      on ARM, atomic_inc does not read the value, it tells the memory subsystem to
      do the increment. Because there is no read in the cpu, there is nothing that
      the read fence can order.
        atomic_inc(&x);
        smp_rmb();
        r1 = READ_ONCE(y);
    - When atomic RMW operations with acquire release semantics, only either
      the read or the write part has the acquire or release semantincs. Think
      of load-conditional (LC) and store-conditional (SC) instructions used to
      perform RMW instead of using a single machine instruction. Either the LC
      or the SC part has the acquire or release semantincs. In particular:
        {}_relaxed: unordered
        {}_acquire: the R of the RMW (or atomic_read) is an ACQUIRE
        {}_release: the W of the RMW (or atomic_set)  is a  RELEASE


RISCV
 - weird AMOSWAP ordering explained https://lore.kernel.org/linux-riscv/82beae6a-2589-6136-b563-3946d7c4fc60@nvidia.com/
   this is mentioned as a general architecture phenomenon in the memory-barriers.txt linux doc!
     *A = a;
     RELEASE M
     ACQUIRE N
     *B = b;
   could occur as:
     ACQUIRE N, STORE *B, STORE *A, RELEASE M 
   but this cannot introduce a deadlock. The cpu is running both the release
   and acquire operations simultaneously in its pipeline. Consider that even if
   the acquire spins, the cpu will eventually execute the release, and, at some
   point, it will indirectly allow the acquire to finish.
 - fence.tso is something like "fence r, rw" + "fence w, w" in a single instruction
