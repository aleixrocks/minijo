DOCUMENTATION
 - Documentation/vm/hugetlbpage.txt
 - Documentation/vm/transhuge.txt
 - LWN Article series
    - https://lwn.net/Articles/374424/
    - https://lwn.net/Articles/375096/
    - https://lwn.net/Articles/376606/
    - https://lwn.net/Articles/378641/
 - Documentation/filesystems/proc.txt

BASIC
 - There are Transparent Huge Pages (THP), and "explicit" Huge Pages.
 - THP are used almost without the intervention of the user but still is
   required some system configuration. It is not always clear that THP are
   actually being used when running an application; the kernel might decide to
   fall back to standard pages if it is running low on Huge Pages, for instance.
 - "explicit" Huge Pages require of explicit coding to be used but there are
   tools to make its usage simpler (libhugetlbfs package/library/toolchain).
   Hence there are two ways to use explicit Huge pages: low level usage and
   libhugetlbfs usage.

EXPLICIT HUGE PAGES
 - Allocate hugepages at system-level: It is possible to allocate either at
   runtime or at boot time (the second is better to avoid fragmentation)
    - Allocate at runtime
        echo 50 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
    - Allocate at boot time
        - Use the following boot parameters
            hugepages=N -> where 'N' is the number of huge pages requested of
            default size default_hugepagesz=<size> -> (optional) default
            huge page size.
        - To allocate huge pages of a specific size, one must precede the huge
          pages boot command parameters with a huge page size selection parameter
            hugepagesz=<size> -> size in [kKmMgG] (e.g. 2M) 
 - Allocate hugepages at application-level
    - Allocate memory with the mmap syscall and the MAP_ANONYMOUS|MAP_HUGETLB
      flags.
 - Check usage of Explicit hugepages with
     - Amount of pages used (AnonHugePages is for THP, not Explicit HP!):
         egrep '^Huge' /proc/meminfo
     - Shows the count of memory mapped areas grouped by the page size that
       they are using.
         grep KernelPageSize: /proc/<pid>/smaps | sort | uniq -c
     - To understand all filed of /proc/<pid>/smaps, read:
         https://www.kernel.org/doc/Documentation/filesystems/proc.txt
     - List total 2MiB pages for a process:
          grep -B 11 'KernelPageSize:     2048 kB' /proc/[PID]/smaps | grep "^Size:" | awk 'BEGIN{sum=0}{sum+=$2}END{print sum/1024}'

EXPLICIT HUGE PAGES WITH libhugetlbfs
 - Documentation: https://github.com/libhugetlbfs/libhugetlbfs/blob/master/HOWTO
 - To avoid the hassle of using malloc directly, libhugeltbfs provides means to
   do this transparently but with some drawbacks.
 - Install libhugetlbfs and libhugetlbfs-utils (the later might be included
   in the former depending on the disto)
 - Mount a hugetlbfs per each page size that you want to use and give it
   the right permisions if non-sudo users want to use it. If the page size
   if not specified, the default one is used (2MiB probably). There might
   already be a hugetlbfs mounted in your system, check it first with
   mount.
     mkdir -p /mnt/hugetlbfs-64K
     mount -t hugetlbfs none -opagesize=64k,uid=<user-name> /mnt/hugetlbfs-64K
 - Allocate pages in the system either as explained in EXPLICIT HUGE PAGES or
   using the libhugeltbfs command:
     hugeadm --pool-pages-min 2MB:50  # allocate 50 pages of 2MiB
 - List current pools
     hugeadm --pool-list
 - Transparently use hugepages when using malloc: This uses a glibc hook to force it to use
      hugepages when allocating new memory. It also prevents malloc from using
      brk. However, it only works for the main Arena, other threads arenas don't
      make use of the morecore hook (provided by libhugetlbfs to allocate more
      memory when needed in malloc). To force all threads to use the main arena
      (effectively limiting the number of arenas to 1), the mallopt
      MALLOC_ARENA_MAX=1 should be used. Be aware that this increases contetion in
      user-space.
    - Do it manually:
        export LD_PRELOAD=libhugetlbfs.so
        export HUGETLB_MORECORE=1G # backup malloc with hugepages
        export HUGETLB_PATH=/mnt/hugetlbfs-64K # needed if multiple hugetlbfs mounted
        command
    - Do it with tools: 
        hugectl --heap <application>
        hugectl --heap=64k ./target-application # use 64k pages instead of default size
  - Transparently use hugepages for code, stack and BSS
      check libhugetlbfs HOWTO file in github
  - Check hugepage usage of an application as explained in "EXPLICIT HUGE PAGES"

TRANSPARENT HUGE PAGES (THP)
 - THP is only enabled for anonymous memory regions.
 - THP do not require to allocate a pool of pages.
 - Size of THP:
     cat /sys/kernel/mm/transparent_hugepage/hpage_pmd_size
 - Transparent Hugepage Support for anonymous memory can be entirely disabled
   (mostly for debugging purposes) or only enabled inside MADV_HUGEPAGE regions
   (to avoid the risk of consuming more memory resources) or enabled system
   wide. This can be achieved with one of:
      echo always >/sys/kernel/mm/transparent_hugepage/enabled
      echo madvise >/sys/kernel/mm/transparent_hugepage/enabled
      echo never >/sys/kernel/mm/transparent_hugepage/enabled
   When set to always the khugepaged is started and when set to never is
   killed. 
 - If no pages are available, the <thp daemon name> will wake up and try to get
   more pages. This behaviour is determined by the
   /sys/kernel/mm/transparent_hugepage/defrag file. Where
     - always: application stalls when there is no memory and starts compacting
       memory to get some THP.
     - madvise (default): Like always but only for mmap areas that have used the
       madvise(MADV_HUGEPAGE).
     - defer: wake up kswapd to reclaim pages and kcompactd to compact memory
       in background so pages are available later.
     - defer+madvise: to defer only for madvise(MADV_HUGEPAGE) regions.
     - never: don't do that
 - The khugepaged runs the defrag algorithm in background regardless of the
   previous option. It can be explicitly disabled.
 - Optional optimizations
    - THP are automatically used when memory is aligned to the page size. In
      userland, no modifications to the applications are necessary (hence
      transparent). But there are ways to optimize its use. For applications that
      want to use hugepages, use of posix_memalign() can also help ensure that
      large allocations are aligned to huge page (2MB) boundaries.
    - Applications that gets a lot of benefit from hugepages and that don't risk
      to lose memory by using hugepages, should use madvise(MADV_HUGEPAGE) on
      their critical mmapped regions. 
 - The khugepaged kernel thread occasionally attempts to substitute smaller
   pages being used currently with a hugepage allocation, thus maximizing THP
   usage. khugepaged will be automatically started when
   transparent_hugepage/enabled is set to "always" or "madvise, and it'll be
   automatically shutdown if it's set to "never"
 - They support swap (huge pages are broken into smaller pages when swapping)
 - MONITORING/DEBUG
    - System wide
       - Number of currently used THP. This does not include explicit huge pages!
           grep AnonHugePages /proc/meminfo 
           AnonHugePages:    632832 kB
    - Per process usage:
       - cat /proc/[PID]/smaps -> show mappings and page usage. Be aware that
         the KernelPageSize: field only applies for explicit huge pages, not
         for transparent huge pages. In fact, a memory mapped area might use
         different sized transparent huge pages (regardless of the explicit
         huge page case, in which a single size per mmaped memory is used).
           - AnonHugePages
    - Other statistics. Check Documentation/vm/transhuge.txt for description.
        egrep 'trans|thp' /proc/vmstat
        nr_anon_transparent_hugepages 2018
        thp_fault_alloc 7302
        thp_fault_fallback 0
        thp_collapse_alloc 401
        thp_collapse_alloc_failed 0
        thp_split 21

MONITORING/DEBUG
 - Per NUMA node default size Huge page distribution
     cat /sys/devices/system/node/node*/meminfo | fgrep Huge
 - Be aware that /proc/meminfo AnonPages includes also AnonHugePages! Hence, if
   you want to see only the total amount of memory backed by 4KiB pages you
   must calculate AnonPages - AnonHugePages
 - /proc/meminfo PageTables: field shows the amount of memory used to hold the
   last level of the page table. A huge number here means that maybe too many
   individual pages have been created. Too many pages, might trigger kernel
   daemons (kscand/kswapd) to scan for free memory. THe more pages, the more
   work this damoens will need to do and more heavy the pressure on the CPU.
   Hence, use huge pages!!!
