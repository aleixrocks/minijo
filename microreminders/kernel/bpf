BASIC
 - The typical workflow is that BPF programs are written in C, compiled by LLVM
   into object / ELF files, which are parsed by user space BPF ELF loaders
   (such as iproute2 or others), and pushed into the kernel through the BPF
   system call. The kernel verifies the BPF instructions and JITs them,
   returning a new file descriptor for the program, which then can be attached
   to a subsystem (e.g. networking). If supported, the subsystem could then
   further offload the BPF program to hardware (e.g. NIC).
 - The kernel as an bpf program interpreter, but if the arch supports it, it
   will run a JIT to increase performance.
 - man bpf-helpers:  for a list of available helpers
 - in deep doc: https://docs.cilium.io/en/stable/bpf/
 - BTF: BPF Trace Format. Similar to DWARF but way more simple and
   space-efficient. Kernels are compiled with it by default.
 - btf exports the definition of ALL data strucutes. it can be read with:
   bpftool btf dump file /sys/kernel/btf/vmlinux format c
 - Kernel bpf test suite under tools/testing/selftests/bpf/
 - ulimit -l unlimited : needed if we run out of lockeable memory
 - The kernel allows unprivileged users to load only two types of BPF programs,
   BPF_PROG_TYPE_SOCKET_FILTER and BPF_PROG_TYPE_CGROUP_SKB. You can see the
   check in the kernel for that condition in kernel/bpf/syscall.c.

Important tunnings
 - /proc/sys/net/core/bpf_jit_harden: enable or disable hardening. If
   performance is needed, disable it. It is used for security (it "randomizes"
   immediate values of instructions to prevent opcode injection)
 - /proc/sys/net/core/bpf_jit_enable: enable jit (1) for performance. Set to 2
   for debugging info. If 0, an interpreter is used instead.
 - /proc/sys/net/core/bpf_jit_kallsyms: set to 1 to enable bpf programs kallsym
   export (address of functions) so it can be used by perf and stack traces. If
   harden is enabled, this is disabled.
 - /proc/sys/kernel/unprivileged_bpf_disabled: Enables or disable unprivileged
   use of the bpf(2) system call. The Linux kernel has unprivileged use of
   bpf(2) enabled by default, but once the switch is flipped, unprivileged use
   will be permanently disabled until the next reboot

Considerations
 - During execution, BPF programs are guaranteed to never get preempted by the
   kernel
 - Limited stack space of maximum 512 bytes. BPF_MAP_TYPE_PERCPU_ARRAY map with
   a single entry can be used in order to enlarge scratch buffer space.
 - global variables can be used to pass info between the kernel and user side
   of the bpf program!
 - BPF programs cannot perform any function calls other than those to BPF
   helpers and compiler built-ins. It is still possible to declare functions.
 - No loops yet, but for "pragma unroll"
 - Tail calls: call into another bpf program. The important detail that it's
   not a normal call, but a tail call. The kernel stack is precious, so this
   helper reuses the current stack frame and jumps into another BPF program
   without adding extra call frame. It's trivially done in interpreter and a
   bit trickier in JITs.
 - Defining const strings or other arrays in the BPF C program does not work
   for the same reasons as pointed out in sections 1 and 3, which is, that
   relocation entries will be generated in the ELF file which will be rejected
   by loaders due to not being part of the ABI towards loaders (loaders also
   cannot fix up such entries as it would require large rewrites of the already
   compiled BPF sequence).

Tools
 - bpftool prog [--pretty]: list all bpf programs loaded
 - bpftool map: list all maps
 - bpftool map dump id <id> : dump the key value pairs for map <id>
    - use BPF_ANNOTATE_KV_PAIR() macro in the source code to annotate the map
      with a struct type for pretty printing.
 - bpftool prog show id <id>: The program of ID 406 is of type sched_cls
   (BPF_PROG_TYPE_SCHED_CLS), has a tag of e0362f5bd9163a0a (SHA sum over the
   instruction sequence), it was loaded by root uid 0 on Apr 09/16:24. The BPF
   instruction sequence is 11,144 bytes long and the JITed image 7,721 bytes.
   The program itself (excluding maps) consumes 12,288 bytes that are accounted
   / charged against user uid 0. And the BPF program uses the BPF maps with IDs
   18, 20, 8, 5, 6 and 14. The latter IDs can further be used to get information
   or dump the map themselves.
 - bpftool prog dump xlated id <id>: dump program <id> assembly after verifier.
   There are some instructions rewrites after the loader step. For instance,
   inlining of helper function.
 - bpftool prog dump jited id <id>: dump jitted version
 - visual execution graph:
    bpftool prog dump xlated id 406 visual &> output.dot
    dot -Tpng output.dot -o output.png
 - For the dumpted program to show the names of function calls correctly:
    echo 0 > /proc/sys/kernel/kptr_restrict
    echo 1 > /proc/sys/net/core/bpf_jit_kallsyms

Debugging
 - bpf_trace_printk() : sends output to kernel trace pipe:
   tail -f /sys/kernel/debug/tracing/trace_pipe
 - perf list | grep bpf : list tracepoints triggered by bpf code

BPF loaders
 - these are programs that can load bpf programs into the kernel. Each might
   bring special features.
    - libbpf
    - iproute2
    - perf

BPF programs
 - libbpc: general one for tracing (not specific network support).
 - XDP (eXpress Data Path). Networking. It runs the BPF program at the earliest
   possible point in software (as soon as the network driver receives the
   packet). Not even an skb has been allocated (native XDP, requires driver
   support). Some network cards allow to run the BPF code INSIDE the network
   card (offloaded XDP)! If the driver has no support, it will run it at a
   generic point, much later in the network stack (generic XDP)
 - tc (transmission control).  Networking. It runs the BPF program at a later
   point in the network stack, when there is already an skb buffer allocated
   and it is possible to do operations on the skb. It also works for both
   ingress (in) and egress (out).

bpftrace
 - doc:
    - tutorial: https://github.com/iovisor/bpftrace/blob/master/docs/tutorial_one_liners.md
    - reference: https://github.com/iovisor/bpftrace/blob/master/docs/reference_guide.md
 - basic
    - bpftrace converts the awk-like provided script to LLVM intermediate
      representation, which is then compiled to BPF
    - be sure to run on at least LLVM 12, as previous versions had a bpftrace
      bug hard to detect.
    - bpftrace vs systemtap: Bpftrace is generally faster, and provides various
      facilities for quick aggregation and reporting that are arguably simpler
      to use than those provided by SystemTap. On the other hand, SystemTap
      provides several distinguishing features such as: generating user-space
      backtraces without the need for frame pointers, accessing function
      arguments and local variables by name, and the ability to probe arbitrary
      statements. 
       src: https://lwn.net/Articles/852112/
    - compile the kernel with CONFIG_DEBUG_INFO_BTF to enable debug symbols.
    - Upon receiving a SIGUSR1 signal, bpftrace will print all maps to the standard output.
        bpftrace -e 'kretprobe:vfs_read { @bytes = hist(retval); }' &
        kill -s USR1 $(pidof bpftrace)
 - things that you cannot do with bpftrace
    - convert an integer to hex (this requires to allocate a string array, and
      you cannot allocate arrays other than maps, but you cannot print entries
      of a map as a single string of characters)

 - bpftrace -l [SEARCH] : list all probes that match the SEARCH pattern.
    - tracepoint: same as perf, lttng, ftrace. They are prefered over kprobes.
      kprobes instrument functions, and functions might change between kernel
      versions. tracepoints are more stable.
        - run bpftrace -lv to list the tracepoint arguments!
    - kprobe/kretprobe: dynamically injected probes. Allow an offset.
       - kprobe instruments the beginning of a funciton (or any point if an offset is added)
       - kretprobe only instruments the exit point. The built-int retval holds the return value if using a kretprobe
       - to get the offset:
           gdb -q /usr/lib/debug/boot/vmlinux-`uname -r` --ex 'disassemble do_sys_open'
           bpftrace -e 'kprobe:do_sys_open+9 { printf("in here\n"); }'
    - uprobe/uretprobe: dynamic user-space program probes.
       - example
          bpftrace -l 'uretprobe:/bin/bash:readline : find available probe points
          objdump -tT /bin/bash: find the available probe points
          bpftrace -e 'uretprobe:/bin/bash:readline { printf("read a line\n"); }'
       - access arguments like arg0,...argN. You need to check the code to know
         what they mean!
       - if the user binary had debug symbols, you can use args->arg_name
       - bpftrace -lv 'uprobe:/bin/bash:rl_set_prompt' : to see available arguments
       - an offset can be added for uprobe obtained with (for instance): objdump -d /bin/bash
    - usdt: static user-space tracepoints
       - bpftrace -l 'usdt:/usr/lib64/libc.so.6:*' : list available points
    - kfunc: more efficient, implemented as bpf trampolines
    - software: the same as perf software events. They provide counts, such as
      "cpu-clocks". A count can be provided, so this probe will trigger every n
      events.
        - bpftrace -e 'software:faults:100 { @[comm] = count(); }'
    - hardware: hardware driven events. Same as perf.
        - bpftrace -e 'hardware:cache-misses:1000000 { @[pid] = count(); }'
    - iter: allows to define a /sys file that can print info of all processes
      and/or files at once. IOW, it iterates all tasks and/or files and runs
      the provided program for each of them. Creates a /sys files that can be
      read to trigger the program.
        - bpftrace -l iter:* -v : list the associated structures (contexes)
        - bpftrace -e 'iter:task:list { printf("%s:%d\n", ctx->task->comm, ctx->task->pid); }'
        - bpftrace -e 'iter:task_file:/sys/fs/bpf/files { printf("%s:%d %s\n", ctx->task->comm, ctx->task->pid, path(ctx->file->f_path)); }'
    - watchpoint/asyncwatchpoint: monitor memory addresses! experimental. Requires arch support
  - bpftrace -lv "struct path" : if BTF is available, it shows the struct definition!
  - bpftrace -e PROG : where PROG is '[BEGIN{}] [events] [pattern] [action] [END{}]'
     - there can be multiple events, but there is a 512 soft limit.
     - BEGIN and END are optional, printed at the beginning and end of execution, respectively
     - pattern defines a condition for the next action to trigger
     - action is a sequence of instructions enclosed by {}
     - multiple pairs of pattern and action can be defined
     - it is possible to write PROG as .bt file, This is useful for long
       programs and when needing to use "include". It can also be made
       executable with the shebang #!/usr/local/bin/bpftrace
  - scratch variables
    - $<variable_name>=<value> : normal variable declaration
    - careful! these are also how we access parameters passed to the bpftrace script!
    - their default value is 0 if none is supplied
  - maps
    - kind of global variables implemeted with bpf maps. They are initialized
      to 0 if used before being assigned.
    - @ is the simples map. e.g. @++
    - maps can be named "@name"
    - all maps are printed at the end of the execution.
    - multiple keys allowed!
       - bpftrace -e 'BEGIN { @[1,2] = 3; printf("%d\n", @[1,2]); clear(@); }'
    - delete(@start[tid]) deletes the variable to avoid it from printing at the end.
    - bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }'
  - built-in variables
    - comm: name of process
    - pid
    - tid
    - uid: user id
    - gid: group id
    - cpu
    - numaid
    - func: name of the traced function
    - curtask: Current task struct as a u64
    - rand: Random number as a u32
    - nsecs: Nanoseconds since boot. This is a high resolution timestamp counter than can be used to time events.
    - args: structure with all arguments (only for tracepoint probes)
    - arg0, arg1, ... argN: the argument of the function for kprobes
    - probe: the name of the probe that triggered the current event.
       -  bpftrace -e 'tracepoint:sched:sched* { @[probe] = count(); } interval:s:5 { exit(); }'
    - kstack, ustack: the kernel and user current stack as a string!
       - # bpftrace -e 'profile:hz:99 { @[kstack] = count(); }'
       - ustack needs the user program to be compiled with frame pointers
         check this to see if it's fixed: https://github.com/iovisor/bpftrace/issues/1744
       - you can also set perf format kstack(perf) and/or limit the frames kstack(5)
    - retval: holds the return value, only for kretprobe
    - $# : number of positional parameters passed to the .bt script (access them with $1, $2, ...)
    - more vars in the reference documentation
  - built-in functions
    - str(): turns a pointer into the string it points to str(args->filename)
    - count(): @ = count() is similar to @++, but it uses a per-cpu map for efficiency.
    - hist(<value>): histogram
    - exit(): exit bpf program
    - clear(map): delete map
    - return: exit the probe, but not the program!
    - system(): execute shell command (WTF). Unsafe option. It forks, execs and
      waits until the command finishes. This will run with the bpf program
      privileges!
    - ksym(), usym(), kaddr(), uaddr(): from symbol to name and viceversa for
      kernel and user pointers.
        -  bpftrace -e 'kprobe:do_nanosleep { printf("%s\n", ksym(reg("ip"))); }'
    - signal(): signal process
    - override(): override return value. This feature only works on kernels
      compiled with CONFIG_BPF_KPROBE_OVERRIDE and only works on functions
      tagged ALLOW_ERROR_INJECTION.
    - cat(): print file
        - bpftrace -e 't:syscalls:sys_enter_execve { printf("%s ", str(args->filename)); cat("/proc/loadavg"); }'
  - search pattern
    - boolean operators are supported
    - bpftrace -e 'tracepoint:syscalls:sys_exit_read /pid == 18644/ { @bytes = hist(args->ret); }'
    - bpftrace -e 'kprobe:vfs_read { @start[tid] = nsecs; } kretprobe:vfs_read /@start[tid]/ { @ns[comm] = hist(nsecs - @start[tid]); delete(@start[tid]); }'
       - here, if /@start[tid]/ does not exist, it will not trigger the kretprobe.
  - intervals
    - this triggers an action on a single cpu only, for example print every 1s:
       - bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @syscalls = count(); } interval:s:1 { print(@syscalls); clear(@syscalls); }'
  - profiles
    - this triggers an acction on all cpus. This creates data for a flame graph
       - bpftrace -e 'profile:hz:99 { @[kstack] = count(); }'
  - include kernel structs and casting
    - example:
        #include <linux/path.h>
        #include <linux/dcache.h>
        kprobe:vfs_open { printf("open path: %s\n", str(((struct path *)arg0)->dentry->d_name.name)); }
    - not all structs are always available, only the ones that are in headers.
      If the kernel has BTF (BPF Type Format) data (CONFIG_DEBUG_INFO_BTF), all
      kernel structs are always available. Use the macro bpftrace
      BPFTRACE_HAVE_BTF macro to detect in-kernel BTF support.
    - if you include something, the in-kernel BTF support will be disabled and
      you will either need to include all or define all by hand!
    - you can define your own structs in the .bt script too!
  - loops. we have unroll and while. continue and break work as expected for while
    - bpftrace -e 'kprobe:do_nanosleep { $i = 1; unroll(5) { printf("i: %d\n", $i); $i = $i + 1; } }'
    - bpftrace -e 'i:ms:100 { $i = 0; while ($i <= 100) { printf("%d ", $i); $i++} exit(); }'
  - tuples
    - once created, they are imutable
    - bpftrace -e 'BEGIN { $t = (1, 2, "string"); printf("%d %d %s\n", $t.0, $t.1, $t.2); }'

HOW TO ADD KPROBES AT FUNCTION OFFSET AND RETREIVE LOCAL VARS
 - see my perf doc on how make this work properly
 - perf probe --line schedule : list available points to install a kprobe
 - perf probe --vars schedule:5 : list available points to install a kprobe
 - perf probe -vn --add 'aleix:test=schedule:5 tsk' : dry run of installing a
   kprobe with verbose info. The verbose info should display the symbol
   function + byte offset and the register to the local variable. The probe
   will be inserted later on by bpf
     Probe point found: schedule+61
     Searching 'tsk' variable in context.
     Converting variable tsk into trace event.
     tsk type is (null).
     Found 1 probe_trace_events.
     Opening /sys/kernel/tracing//kprobe_events write=1
     Writing event: p:aleix/test _text+11655757 tsk=%bx:x64
 - bpftrace -e 'kprobe:schedule+61 { $var=reg("bx"); @map[tid, $var] = count()}' :
   manual install a bpf probe using the previous info. Note, $var
   holds a pointer here, but we cannot convert it to a hex string to use it as
   a map key, so the map will print long integer values as keys instead of nice
   hex values :(
 - perf probes do not work directly with bpf because as explained here:
   https://github.com/iovisor/bcc/issues/2150
 - Optionally, the probe info is also here (but relative to _text!): cat
   /sys/kernel/debug/tracing/kprobe_events : read the func byte offset and var
   register

LIBBPF
 - getting started:
    - code for getting started: https://nakryiko.com/posts/libbpf-bootstrap/
    - examples: https://github.com/iovisor/bcc/tree/master/libbpf-tools
    - BCC to libbpf conversion guide: https://nakryiko.com/posts/bcc-to-libbpf-howto-guide/
    - BPF CO-RE (Compile Once – Run Everywhere) https://nakryiko.com/posts/bpf-portability-and-co-re/
    - libbpf and co-re reference: https://nakryiko.com/posts/bpf-core-reference-guide/
    - cilium doc with generic intro and lots of xdp and tc details: https://docs.cilium.io/en/latest/bpf/
 - multiple bpf programs can be defined in the same C file, maps and globals
   are shared among all these.
 - you might need to generate a header with all kernel data type definitions.
   BTF exports that info at runtime and can be read by:
     bpftool btf dump file /sys/kernel/btf/vmlinux format c
 - After compiling the bpf code, the tools generate a <name>.skel.h header.
   This file includes strucutres that the user code can use to open, load,
   destroy and communicate with the bpf application. See
   https://nakryiko.com/posts/libbpf-bootstrap/  for how this works.
 - includes:
     #include "vmlinux.h"               /* all kernel types */
     #include <bpf/bpf_helpers.h>       /* most used helpers: SEC, __always_inline, etc */
     #include <bpf/bpf_core_read.h>     /* for BPF CO-RE helpers */
     #include <bpf/bpf_tracing.h>       /* for getting kprobe arguments */
 - map declaration examples
     // array map
     struct {
         __uint(type, BPF_MAP_TYPE_ARRAY);
         __uint(max_entries, 128);
         __type(key, u32);
         __type(value, struct my_value);
     } my_array_map SEC(".maps");
     
     // hash map
     struct {
         __uint(type, BPF_MAP_TYPE_HASH);
         __uint(max_entries, 10240);
         __type(key, u32);
         __type(value, struct my_value);
     } my_hash_map SEC(".maps")

     // per-cpu array
     struct {
         __uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);
         __uint(max_entries, 1);
         __type(key, u32);
         __type(value, struct my_value);
     } heap SEC(".maps");

   for other maps see ???

-  BPF programs are placed in special sections of the resulting ELF file. To
   do so, we use  constructs like SEC("tp/syscalls/sys_enter_write") int
   handle_tp(void *ctx) { ... } to mark function that will be loaded into the
   kernel as bpf programs.  Section name defines what type of BPF program
   libbpf should create and how/where it could be attached in the kernel. In
   this case, we define a tracepoint BPF program, which will be called each
   time a write() syscall is invoked from any user-space application.
   Follws an incomplete list of types of bpf programs:
    - all available kinds of "SEC" can be seen here
      https://github.com/libbpf/libbpf/blob/787abf721ec8fac1a4a0a7b075acc79a927afed9/src/libbpf.c#L7935-L8075
      also, please see examples in 
        https://github.com/iovisor/bcc/tree/master/libbpf-tools

    - tp_btf/<name> : tracepoint, apparently the most popular option for
      tracepoints, but I don't know why it is better.
        SEC("tp_btf/sched_switch")
        int BPF_PROG(sched_switch, bool preempt, struct task_struct *prev, struct task_struct *next) 
    - raw_tp/<name> for raw tracepoint : No idea how it differs from the
      others, but it should be bettern than tp/tracepoint as explained in
      "BCC to libbpf conversion guide" article.
        TODO
    - tp/<category>/<name> or tracepoint/<category>/<name>for tracepoints. The
      argument is generally named "struct trace_event_raw_<name> *", but if it
      doesn't work, check for the definition in vmlinux.h
        SEC("tracepoint/sched/sched_process_exec")
        int tracepoint__sched__sched_process_exec(struct trace_event_raw_sched_process_exec *args)
    - fentry/fexit : If available, use this instead of kprobes for attaching to
      kernel functions. They are more efficient as explained in "BCC to libbpf
      conversion guide" article. I'm not sure if we can have arguments here.
        SEC("fentry/vfs_create")                                                                                                            
        int BPF_PROG(fentry_vfs_create)                                                                                                     

        SEC("fexit/do_page_cache_ra")
        int BPF_PROG(do_page_cache_ra_ret)
    - kprobe/<func_name> for kprobe and kretprobe/<func_name> for kretprobe.
      Use BPF_KPROBE and BPF_KRETPROBE to be able to use arguments. I don't
      know how to set an offset here, if possible.
        SEC("kprobe/acct_collect")
        int BPF_KPROBE(kprobe__acct_collect, long exit_code, int group_dead)
    - cgroup_skb/ingress, cgroup_skb/egress, and a whole family of
      cgroup/<subtype> programs.
 - Also note that Syscall functions got renamed in 4.17 kernels. Starting from
   4.17 version, syscall kprobe that used to be called, say, sys_kill, is
   called now __x64_sys_kill (on x64 systems, other architectures will have
   different prefix, of course). 
 - global variables. They can be const or non-const.
    - non-const globals can be used to communicate user-kernel and between bpf
      programs.  Do not use the static keyword because of reasons. There is no
      need to add volatile, generally.These variables must be initialized or
      will fail at load time.
        - bpf code:
            int my_global_var;
        - user code:
            skel = bootstrap_bpf__open();
            skel->data->my_global_var // for initialized data
              or 
            skel->bss->my_global_var // for zero initialized data

    - const globals can be used for configuration. The user app, first opens
      the bpf application, then modifies the values of the const variables, and
      then loads the program into the kernel. From that point, the values
      cannot be changed. They must be volatile or the compiler might remove
      them. These variables must be initialized or will fail at load time.
        - bpf code:
           const volatile struct {
               bool feature_enabled;
               int pid_to_filter;
           } my_cfg = {};

        - user code:
            struct <name> *skel = <name>__open();
            if (!skel)
                /* handle errors */
            
            skel->rodata->my_cfg.feature_enabled = true;
            skel->rodata->my_cfg.pid_to_filter = 123;
            
            if (<name>__load(skel))
                /* handle errors */
    - In both cases, to use them, just access them normally.
        if (my_cfg.feature_enabled) {
 - for loops must be unrolled
    #pragma unroll
    for (i = 0; i < 10; i++) { ... }
 - static functions on kernels older than 4.16 have to be marked as alway
   inlined with static __always_inline. Otherwise, its even better for static
   functions to be declared __noinline, "which quite often will improve code
   generation and will avoid some of the common BPF verification failures due
   to unwanted register-to-stack spilling"
 - Non-inlined global functions are also supported starting from 5.5 kernels,
   but they have different semantics and verification constraints than static
   functions. Make sure to check them out as well!
 - bpf_printk("BPF triggered from PID %d.\n", pid); emits message to
   /sys/kernel/debug/tracing/trace_pipe It accepts printf-like format string
   and can handle only up to 3 arguments. It's quite expensive, making it
   unsuitable to be used in production. Use only for debugging.
     char comm[16];
     u64 ts = bpf_ktime_get_ns();
     u32 pid = bpf_get_current_pid_tgid();
     
     bpf_get_current_comm(&comm, sizeof(comm));
     bpf_printk("ts: %lu, comm: %s, pid: %d\n", ts, comm, pid);
 - user-kernel communication using maps:
    - BPF_MAP_TYPE_RINGBUF : map of type bpf ringbuffer. This is a
      multiple-producer single-consumer (MPSC) ciruclar buffer of fixed size.
       - ring_buffer__new(): create buffer from user-space side. You can supply
         a callback that will be called for every written event.
       - ring_buffer__poll(): poll events from user-space side.
       - There are two options for writting an element using bpf ringbuffer. output and reserve/submit:
          - bpf_ringbuf_output() : write into the buffer. Allows writting
            elements of different sizes.
          - bpf_ringbuf_reserve(): first reserve space for an element. Fill it,
            and then call commit. The size must be set at compile-time, no
            variable sizes allowed.
          - bpf_ringbuf_submit(): submit a previously reserved element.
     - BPF_MAP_TYPE_PERCPU_ARRAY: (probably not what you want) Per-cpu circular
       buffer. Events can be unordered.  Cannot reserve memory prior to filling
       it, needs to use an auxiliar chunk of memory. Allows data of different
       sizes. An array per cpu performs better than a MCSP when millions of
       events are generated per second, but it is possible have a per-cpu MCSP
       bpf ring.
 - BPF CO-RE
    - compile once, run everywhere. In essence, CO-RE is about reading kernel
      data inside a bpf program in a portable way. This is the new framwork
      which deprecates bcc. The whole idea is that you no longer need to
      distribute a compiler embedded in your bpf program and don't need to
      compile at initialization time with it. Instead, lots of relocation info
      are added when compiling and uses BTF to smartly deal with symbol offsets
      that vary between kernel versions (think of a pid member of struct
      task_struct that migth change its position, name or meaning in differnt
      kernel version). CO-RE provides ways to make this portable, sometimes
      with automation, sometimes with tools that allows the developer to
      manually write alternatives (ifs) dependeing on the kernel version. 
    - doc
       - reference https://nakryiko.com/posts/bpf-core-reference-guide/
       - intro https://nakryiko.com/posts/bpf-portability-and-co-re/
       - api: https://github.com/libbpf/libbpf/blob/master/src/bpf_core_read.h
    - Note, there are some type of bpf programms like (tp_btf,
      kfunc/fentry,kretfunc/fexit,fmod_ret, LSM) that do not need to use the following
      bpf_core_read() functions and variants, you can access the memory
      directly. You sill need the *_str versions through. See the reference
      above for more info.
    - bpf_core_read(&parent_task, sizeof(void *), &task->parent); : read field
      from kernel. It stores relocation info on the field offset that is later
      filled by the loader (libbpf) at runtime using the system's BTF info. Be
      careful with size, as this is not recorded in the relocation. You need to
      explicitly control it.
    - bpf_core_read_str(): read a null terminated string. stores reloc info as
      well.  be aware, this only reads explicit c arrays like var[3] but not
      pointers to strings of texts. If you want to read a char * string, first
      do a bpf_core_read of the pointer (which stores reloc data), and once you
      have the pointer, call bpf_probe_read_kernel_str() instead (note,
      "probe", not "core"!) because we no longer need reloc data for this case,
      we already have the offset in the previous read!
    - name = BPF_CORE_READ(t, mm, exe_file, fpath.dentry, d_name.name) : read a
      chain of pointers. This example is equivalent to name =
      t->mm->exe_file->fpath.dentry->d_name.name; if there is any error, it
      will return 0. If that can be confused with the original value, you need
      to do a bpf_core_read for each dereference!
    - err = BPF_CORE_READ_INTO(&name, ...): it returns an error code instead.
    - BPF_CORE_READ_STR_INTO(): For cases when the last field is a character array field
    - BPF_CORE_READ_BITFIELD() and BPF_CORE_READ_BITFIELD_PROBED(): read a
      bitfield (in c, structs like field:2). Probed is used in the same cases
      as BPF_CORE_READ is needed (reloc info). These return return properly
      signed-extended 8-byte integers back (work with any integer, not only
      bitfields).
    - bpf_core_enum_value(): for bitfields that are auto-generated at compile
      time based on enabled CONFIG_*, like enum cgroup_subsys_id
    - bpf_core_type_size(), bpf_core_field_size(): return size of variable. It
      can be used to bpf_core_read to make the call fully relocatable, for
      example.
    - bpf_core_field_exists(): used to detect if a field exist. the field
      itself is never read, so it is safe to do: bpf_core_field_exists(((union
      bpf_attr *)0)->link_create.perf_event.bpf_cookie). If used in an if, the
      non-taken branch will never be validated and treated as dead-code.
    - bpf_core_type_exists(struct bpf_ringbuf): check if the type itself exist.
      But seems tricky to use because of forward declarations. Better check the
      reference manual before using.
    - bpf_core_enum_value_exists(): detect if enums exist, it can be used to
      detect the availability of bpf helpers.
    - extern int LINUX_KERNEL_VERSION __kconfig : include the kernel version for feature checks.
      if (LINUX_KERNEL_VERSION > KERNEL_VERSION(5, 15, 0)) {
    - you can also check for CONFIG_* macros with extern keyword but it's a bit
      tricky. Better check the docs
    - in cases where including vmlinux.h does not contain the definition that we want, we can declare the struct that we want with only the fields that we will use. The order is not important either. For example, for struct task_struct, we can define:
        struct task_struct {
            int pid;
            char comm[16];
            struct task_struct *group_leader;
        } __attribute__((preserve_access_index));
      preserve_access_index is used for accesing members directly (without
      bpf_core_read) for the types of bpf programs that allow it. But it's ok
      to use it always for all kinds of bpf programs.
    - For cases where a kernel struct definition changes, we can define both
      versions of the structs in our bpf program. if the suffix <name>___sufix
      is added, the verifier will relocate <name>___sufix against <name>. There
      we can do:
        struct task_struct {
            int __state;
        } __attribute__((preserve_access_index));
        
        struct task_struct___old {
            long state;
        } __attribute__((preserve_access_index));
      and then cast the appropiate one:
        struct task_struct *t = (void *)bpf_get_current_task();
        int state;
        
        if (bpf_core_field_exists(t->__state)) {
            state = BPF_CORE_READ(t, __state);
        } else {
            /* recast pointer to capture task_struct___old type for compiler */
            struct task_struct___old *t_old = (void *)t;
        
            /* now use old "state" name of the field */
            state = BPF_CORE_READ(t_old, state);
        }
     - bpf_core_read_user() and family: same as the non-user part. Allow to
       read a kernel var form user space.
          
